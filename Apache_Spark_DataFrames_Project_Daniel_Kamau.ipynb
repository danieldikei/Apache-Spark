{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Apache Spark DataFrames Project**\n",
        "\n",
        "**Instructions**\n",
        "As a Data professional, you need to perform an analysis by answering questions about some stock market data on Safaricom from the years 2012-2017."
      ],
      "metadata": {
        "id": "Ye_icExcyIIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Importation and Exploration**\n"
      ],
      "metadata": {
        "id": "UMUcIJqHy6-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing pyspark\n",
        "# ---\n",
        "#\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk2OZkzcydoR",
        "outputId": "d90224b5-8061-424d-c9c9-5dc166f40ef3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=66a904a4f27a549f2ed87ebceb85cb0989c3bfe9af814fda4810beb7b79ef498\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We run a local spark session\n",
        "# ---\n",
        "#\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "t1FdwPTJyzMe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloaded the stock csv file from https://bit.ly/3pmchka\n",
        "#-----\n",
        "#reading file and printing the column names\n",
        "\n",
        "saf_stock = open('saf_stock.csv')\n",
        "saf_stock.readline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hl3fbYCMzNBF",
        "outputId": "b0e9406a-bf8c-426e-c1f5-d4a251e95b34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Date,Open,High,Low,Close,Volume,Adj Close\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the stocks file and infer the data types\n",
        "#Making Observations about the schema\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sqlCtx = SQLContext(sc)\n",
        "\n",
        "# Read csv data into a DataFrame object `saf_df`\n",
        "\n",
        "saf_df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"saf_stock.csv\")\n",
        "\n",
        "#Making Observations about the schema\n",
        "saf_df.printSchema()\n",
        "saf_df.show()\n",
        "\n",
        "# Print the type\n",
        "#print(type(saf_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpEJNGto0oX0",
        "outputId": "0631751a-eebb-42ca-a5ac-4521b19d608b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n",
            "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "|               Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
            "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "|2012-01-03 00:00:00|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
            "|2012-01-04 00:00:00|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
            "|2012-01-05 00:00:00|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
            "|2012-01-06 00:00:00|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
            "|2012-01-09 00:00:00|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
            "|2012-01-10 00:00:00|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
            "|2012-01-11 00:00:00|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
            "|2012-01-12 00:00:00|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
            "|2012-01-13 00:00:00|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
            "|2012-01-17 00:00:00|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
            "|2012-01-18 00:00:00|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
            "|2012-01-19 00:00:00|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
            "|2012-01-20 00:00:00|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
            "|2012-01-23 00:00:00|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
            "|2012-01-24 00:00:00|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
            "|2012-01-25 00:00:00|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
            "|2012-01-26 00:00:00|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
            "|2012-01-27 00:00:00|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
            "|2012-01-30 00:00:00|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
            "|2012-01-31 00:00:00|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
            "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Open: string, High: string, Low: string, Close: string, Volume: string, Adj Close: string]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the desscribe method to learn about the data frame"
      ],
      "metadata": {
        "id": "uNW9AkLBOLAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(saf_df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq-hsAN4OUtH",
        "outputId": "781bd0c3-90f4-4626-b828-ec62ce7cd6f8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[summary: string, Open: string, High: string, Low: string, Close: string, Volume: string, Adj Close: string]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import format_number\n",
        "\n",
        "result = saf_df.describe()\n",
        "print(result.select(result['summary'],\n",
        "              format_number(result['Open'].cast('float'),2).alias('Open'),\n",
        "              format_number(result['High'].cast('float'), 2).alias('High'),\n",
        "              format_number(result['Low'].cast('float'), 2).alias('Low'),\n",
        "              format_number(result['Close'].cast('float'), 2).alias('Close'),\n",
        "              result['Volume'].cast('int').alias('Volume'),\n",
        "              format_number(result['Adj Close'].cast('float'), 2).alias('Adj Close')\n",
        "              ).show())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBYinfRNdXS-",
        "outputId": "d4f8267c-dcec-4684-e163-a8e0ff6fca22"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+--------+--------+--------+--------+---------+\n",
            "|summary|    Open|    High|     Low|   Close|  Volume|Adj Close|\n",
            "+-------+--------+--------+--------+--------+--------+---------+\n",
            "|  count|1,258.00|1,258.00|1,258.00|1,258.00|    1258| 1,258.00|\n",
            "|   mean|   72.36|   72.84|   71.92|   72.39| 8222093|    67.24|\n",
            "| stddev|    6.77|    6.77|    6.74|    6.76| 4519780|     6.72|\n",
            "|    min|   56.39|   57.06|   56.30|   56.42| 2094900|    50.36|\n",
            "|    max|   90.80|   90.97|   89.25|   90.47|80898100|    84.91|\n",
            "+-------+--------+--------+--------+--------+--------+---------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saf_df2 = saf_df.withColumn(\"HV Ratio\",saf_df['High']/saf_df['Volume'])\n",
        "print(saf_df2.select('HV Ratio').show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDs7hwU_fRWW",
        "outputId": "13eace51-980e-499e-d78b-96b3efcb5b21"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            HV Ratio|\n",
            "+--------------------+\n",
            "|4.819714653321546E-6|\n",
            "|6.290848613094555E-6|\n",
            "|4.669412994783916E-6|\n",
            "|7.367338463826307E-6|\n",
            "|8.915604778943901E-6|\n",
            "|8.644477436914568E-6|\n",
            "|9.351828421515645E-6|\n",
            "| 8.29141562102703E-6|\n",
            "|7.712212102001476E-6|\n",
            "|7.071764823529412E-6|\n",
            "|1.015495466386981E-5|\n",
            "|6.576354146362592...|\n",
            "| 5.90145296180676E-6|\n",
            "|8.547679455011844E-6|\n",
            "|8.420709512685392E-6|\n",
            "|1.041448341728929...|\n",
            "|8.316075414862431E-6|\n",
            "|9.721183814992126E-6|\n",
            "|8.029436027707578E-6|\n",
            "|6.307432259386365E-6|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What day had the Peak High in Price?"
      ],
      "metadata": {
        "id": "G1PNvCCLf6OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(saf_df.orderBy(saf_df['High'].desc()).head(1)[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlHzzLVxfy25",
        "outputId": "ceb2c481-4a42-4f66-e74a-147147af0dbf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2015-01-13 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mdWwbo-7g-_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "● What is the mean of the Close column?"
      ],
      "metadata": {
        "id": "P4vsXbIFg9CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "print(saf_df.select(mean('Close')).show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGqKJTQRhBq-",
        "outputId": "52e6280d-abc0-421c-90ea-bb52d4e77c45"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844998012726|\n",
            "+-----------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the max and min of the Volume column?"
      ],
      "metadata": {
        "id": "uaN6aNhyhUk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max, min\n",
        "\n",
        "print(saf_df.select(max('Volume'),min('Volume')).show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5IpkfF4hbeu",
        "outputId": "b322ba78-2cc7-49e8-b0fa-fb51ef7ab76b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|   80898100|    2094900|\n",
            "+-----------+-----------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many days was the Close lower than 60 dollars?"
      ],
      "metadata": {
        "id": "K3-lvhoahtAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(saf_df.filter(saf_df['Close'] < 60).count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjBY6oREhudf",
        "outputId": "61d58cf7-e537-4302-8947-194822f13799"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What percentage of the time was the High greater than 80 dollars?\n"
      ],
      "metadata": {
        "id": "U7NJp2LOh_ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print((saf_df.filter(saf_df['High']>80).count()/saf_df.count()) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDBrpvFkiB0u",
        "outputId": "1acdd658-f53b-4d90-b4f8-42e2214a5728"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.141494435612083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the Pearson correlation between High and Volume?"
      ],
      "metadata": {
        "id": "H52yvU-ziPZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import corr\n",
        "\n",
        "print(saf_df.select(corr('High','Volume')).show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZN0ckIHiQkf",
        "outputId": "00fd0de4-2867-4709-a2d4-1bdc56843f48"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "| corr(High, Volume)|\n",
            "+-------------------+\n",
            "|-0.3384326061737161|\n",
            "+-------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the max High per year?"
      ],
      "metadata": {
        "id": "2hRmYwlcif0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year\n",
        "\n",
        "saf_df.withColumn(\"Year\",year(saf_df['Date'])).groupBy('Year').max().select('Year','max(High)').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN1uvnj3ih6-",
        "outputId": "bb5b4aa6-0b5a-4e78-886a-bb36651390f5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|Year|max(High)|\n",
            "+----+---------+\n",
            "|2015|90.970001|\n",
            "|2013|81.370003|\n",
            "|2014|88.089996|\n",
            "|2012|77.599998|\n",
            "|2016|75.190002|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the average Close for each Calendar Month?"
      ],
      "metadata": {
        "id": "eUIPsOGAjgWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month\n",
        "\n",
        "saf_df.withColumn('Month',month('Date')).select(['Month','Close']).groupBy('Month').mean().select('Month','avg(Close)').orderBy('Month').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1drct7yjh4X",
        "outputId": "0a90d30d-c08f-4184-b986-5ef0c1061365"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|Month|       avg(Close)|\n",
            "+-----+-----------------+\n",
            "|    1|71.44801958415842|\n",
            "|    2|  71.306804443299|\n",
            "|    3|71.77794377570092|\n",
            "|    4|72.97361900952382|\n",
            "|    5|72.30971688679247|\n",
            "|    6| 72.4953774245283|\n",
            "|    7|74.43971943925233|\n",
            "|    8|73.02981855454546|\n",
            "|    9|72.18411785294116|\n",
            "|   10|71.57854545454543|\n",
            "|   11| 72.1110893069307|\n",
            "|   12|72.84792478301885|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}